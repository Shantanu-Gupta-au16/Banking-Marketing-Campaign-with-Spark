{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT742Task2-Shantanu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.5.5",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shantanu9326/Bank-Dataset/blob/master/SIT742Task2_Shantanu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_th7e2nu0At7",
        "colab_type": "text"
      },
      "source": [
        "# SIT742: Modern Data Science \n",
        "**(Assessment Task 02: Bank Marketing Data Analytics)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Project Group Information:**\n",
        "\n",
        "- Names:\n",
        "- Student IDs:\n",
        "- Emails:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eASNvREtBU9G"
      },
      "source": [
        "# 1.Import Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "duCJfMrnGu00",
        "cellView": "code",
        "outputId": "5be79304-6197-4d27-a838-97444e9cbded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install wget\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aGOo805LA-fM",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K7bEtO_fBZmE"
      },
      "source": [
        "# 2.Read and check data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADPspOv0Auh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Assessment/2019/data/bank.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiM-PiiR0Aup",
        "colab_type": "code",
        "outputId": "c1dd33b5-7f8c-4119-ddf2-daf4de9d8efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bank.csv  sample_data  spark-2.4.0-bin-hadoop2.7  spark-2.4.0-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQ8Ts9eZBA-M",
        "outputId": "244c30a8-89d5-495e-f4c2-deb93b11b88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Import the 'bank.csv' as a Spark dataframe and name it as df\n",
        "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
        "df = spark.read.csv('bank.csv', header = True, inferSchema = True) \n",
        "df.printSchema() \n",
        "df.show(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n",
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|       job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 59|    admin.|married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
            "| 56|    admin.|married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
            "| 41|technician|married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
            "| 55|  services|married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
            "| 54|    admin.|married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XJGWVc0yB0UA",
        "outputId": "d9614a01-ba4b-40e6-9875-3ea625872f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Check data distribution\n",
        "# You may use printSchema()\n",
        "df.describe()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, age: string, job: string, marital: string, education: string, default: string, balance: string, housing: string, loan: string, contact: string, day: string, month: string, duration: string, campaign: string, pdays: string, previous: string, poutcome: string, deposit: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrw5M73j6xxt",
        "colab_type": "code",
        "outputId": "0f9fc5f8-60af-438f-b2b9-f31168b2632c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Di5k1SV7Dun",
        "colab_type": "code",
        "outputId": "4efe35c8-fc5f-4f95-c141-165baea96349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age',\n",
              " 'job',\n",
              " 'marital',\n",
              " 'education',\n",
              " 'default',\n",
              " 'balance',\n",
              " 'housing',\n",
              " 'loan',\n",
              " 'contact',\n",
              " 'day',\n",
              " 'month',\n",
              " 'duration',\n",
              " 'campaign',\n",
              " 'pdays',\n",
              " 'previous',\n",
              " 'poutcome',\n",
              " 'deposit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJpWDgy7HgF",
        "colab_type": "code",
        "outputId": "f42f078b-9148-405e-89ec-070e17351d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
            "| 56|     admin.| married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
            "| 41| technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
            "| 55|   services| married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
            "| 54|     admin.| married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
            "| 42| management|  single| tertiary|     no|      0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|    yes|\n",
            "| 56| management| married| tertiary|     no|    830|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|    yes|\n",
            "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|    yes|\n",
            "| 37| technician| married|secondary|     no|      1|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|    yes|\n",
            "| 28|   services|  single|secondary|     no|   5090|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|    yes|\n",
            "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|    yes|\n",
            "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|    yes|\n",
            "| 29| management| married| tertiary|     no|    199|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|    yes|\n",
            "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|    yes|\n",
            "| 31| technician|  single| tertiary|     no|    703|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|    yes|\n",
            "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|    yes|\n",
            "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|    yes|\n",
            "| 49|   services| married|secondary|     no|     -8|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|    yes|\n",
            "| 41|     admin.| married|secondary|     no|     55|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|    yes|\n",
            "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|    yes|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKQMhrtFChHa"
      },
      "source": [
        "# 3.Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VU5xMqN_RyM2",
        "colab": {}
      },
      "source": [
        "#Select features ('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit') as df2\n",
        "df2=df.select(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvQ1k-WjuC39",
        "colab_type": "code",
        "outputId": "04c01bda-89bd-44ed-9025-d06aa6ee010a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df2.columns"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age',\n",
              " 'job',\n",
              " 'marital',\n",
              " 'education',\n",
              " 'default',\n",
              " 'balance',\n",
              " 'housing',\n",
              " 'loan',\n",
              " 'campaign',\n",
              " 'pdays',\n",
              " 'previous',\n",
              " 'poutcome',\n",
              " 'deposit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbBRgJI_tKBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.createOrReplaceTempView(\"bank\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxTBtqWytVFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SQL').getOrCreate()\n",
        "sql_results=spark.sql(\"SELECT * FROM bank WHERE job!='unknown' AND education!='unknown' AND (poutcome == 'failure' OR poutcome == 'success')\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP9eYrbKkgRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=sql_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWm3NMGIB7JE",
        "colab_type": "code",
        "outputId": "45a08645-d855-4b55-d0de-6a45f6c11737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "df2.show(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+\n",
            "|age|         job| marital|education|default|balance|housing|loan|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+\n",
            "| 33|    services| married|secondary|     no|   3444|    yes|  no|       1|   91|       4| failure|    yes|\n",
            "| 56|  technician| married|secondary|     no|    589|    yes|  no|       1|  147|       2| success|    yes|\n",
            "| 34|      admin.| married| tertiary|     no|    899|    yes|  no|       1|  170|       3| failure|    yes|\n",
            "| 53|     retired| married| tertiary|     no|   2269|     no|  no|       2|  150|       1| success|    yes|\n",
            "| 37|  technician| married|secondary|     no|   5115|    yes|  no|       2|  171|       4| failure|    yes|\n",
            "| 45|entrepreneur| married|secondary|     no|    781|     no| yes|       2|  126|       2| failure|    yes|\n",
            "| 46|  unemployed|divorced|secondary|     no|   3354|    yes|  no|       1|  174|       1| success|    yes|\n",
            "| 40|  management| married| tertiary|     no|   3352|    yes|  no|       2|   27|       1| success|    yes|\n",
            "| 32|  technician| married| tertiary|     no|   4654|    yes| yes|       1|  128|       2| failure|    yes|\n",
            "| 30| blue-collar| married|secondary|     no|    501|    yes| yes|       1|  177|       1| failure|    yes|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_T8qvxzR-oI",
        "colab": {}
      },
      "source": [
        "#Covert categorical features to metric features using One hot encoding\n",
        "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
        "                                OneHotEncoder,StringIndexer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWzSGKEgFnye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "de7eb4f1-6c57-4089-c62b-bf8e47548601"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "from pyspark.ml import Pipeline\n",
        "cat_vars=['job','marital','loan','default','education','housing','poutcome','deposit']\n",
        "indexers=[StringIndexer(inputCol=c,outputCol=\"{0}_indexed\".format(c)) for c in cat_vars]\n",
        "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))for indexer in indexers]\n",
        "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "model=pipeline.fit(df2)\n",
        "transformed=model.transform(df2)\n",
        "transformed.show(50)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+-----------+---------------+------------+---------------+-----------------+---------------+----------------+---------------+-------------------+-----------------------+--------------------+-----------------------+-------------------------+-----------------------+------------------------+-----------------------+--------------------+\n",
            "|age|          job| marital|education|default|balance|housing|loan|campaign|pdays|previous|poutcome|deposit|job_indexed|marital_indexed|loan_indexed|default_indexed|education_indexed|housing_indexed|poutcome_indexed|deposit_indexed|job_indexed_encoded|marital_indexed_encoded|loan_indexed_encoded|default_indexed_encoded|education_indexed_encoded|housing_indexed_encoded|poutcome_indexed_encoded|deposit_indexed_encoded|            features|\n",
            "+---+-------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+-----------+---------------+------------+---------------+-----------------+---------------+----------------+---------------+-------------------+-----------------------+--------------------+-----------------------+-------------------------+-----------------------+------------------------+-----------------------+--------------------+\n",
            "| 33|     services| married|secondary|     no|   3444|    yes|  no|       1|   91|       4| failure|    yes|        5.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[5],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[5,10,12,13,1...|\n",
            "| 56|   technician| married|secondary|     no|    589|    yes|  no|       1|  147|       2| success|    yes|        1.0|            0.0|         0.0|            0.0|              0.0|            1.0|             1.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "| 34|       admin.| married| tertiary|     no|    899|    yes|  no|       1|  170|       3| failure|    yes|        2.0|            0.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[2],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[2,10,12,13,1...|\n",
            "| 53|      retired| married| tertiary|     no|   2269|     no|  no|       2|  150|       1| success|    yes|        4.0|            0.0|         0.0|            0.0|              1.0|            0.0|             1.0|            0.0|     (10,[4],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|               (1,[],[])|          (1,[0],[1.0])|(19,[4,10,12,13,1...|\n",
            "| 37|   technician| married|secondary|     no|   5115|    yes|  no|       2|  171|       4| failure|    yes|        1.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "| 45| entrepreneur| married|secondary|     no|    781|     no| yes|       2|  126|       2| failure|    yes|        9.0|            0.0|         1.0|            0.0|              0.0|            0.0|             0.0|            0.0|     (10,[9],[1.0])|          (2,[0],[1.0])|           (1,[],[])|          (1,[0],[1.0])|            (2,[0],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[9,10,13,14,1...|\n",
            "| 46|   unemployed|divorced|secondary|     no|   3354|    yes|  no|       1|  174|       1| success|    yes|        7.0|            2.0|         0.0|            0.0|              0.0|            1.0|             1.0|            0.0|     (10,[7],[1.0])|              (2,[],[])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[7,12,13,14,1...|\n",
            "| 40|   management| married| tertiary|     no|   3352|    yes|  no|       2|   27|       1| success|    yes|        0.0|            0.0|         0.0|            0.0|              1.0|            1.0|             1.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 32|   technician| married| tertiary|     no|   4654|    yes| yes|       1|  128|       2| failure|    yes|        1.0|            0.0|         1.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|           (1,[],[])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,13,15,1...|\n",
            "| 30|  blue-collar| married|secondary|     no|    501|    yes| yes|       1|  177|       1| failure|    yes|        3.0|            0.0|         1.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[3],[1.0])|          (2,[0],[1.0])|           (1,[],[])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[3,10,13,14,1...|\n",
            "| 46|   technician| married| tertiary|     no|      0|     no|  no|       1|  167|       1| failure|    yes|        1.0|            0.0|         0.0|            0.0|              1.0|            0.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "| 38| entrepreneur| married| tertiary|     no|   1110|    yes|  no|       2|  183|       2| failure|    yes|        9.0|            0.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[9],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[9,10,12,13,1...|\n",
            "| 32|     services| married|secondary|     no|    983|    yes|  no|       2|  133|       1| failure|    yes|        5.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[5],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[5,10,12,13,1...|\n",
            "| 31|   unemployed| married|secondary|     no|    314|    yes|  no|       3|  178|       7| failure|    yes|        7.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[7],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[7,10,12,13,1...|\n",
            "| 50|  blue-collar| married|  primary|     no|  12519|    yes|  no|       3|   34|       1| failure|    yes|        3.0|            0.0|         0.0|            0.0|              2.0|            1.0|             0.0|            0.0|     (10,[3],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|                (2,[],[])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[3,10,12,13,1...|\n",
            "| 47|   technician| married|secondary|     no|      0|     no|  no|       1|   10|       1| failure|    yes|        1.0|            0.0|         0.0|            0.0|              0.0|            0.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "| 59|   management| married| tertiary|     no|   7049|     no|  no|       1|  163|       2| failure|    yes|        0.0|            0.0|         0.0|            0.0|              1.0|            0.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 31|   management| married|secondary|     no|   8629|    yes|  no|       1|  184|       2| failure|    yes|        0.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 53|  blue-collar| married|secondary|     no|   1777|    yes|  no|       5|  154|       1| failure|    yes|        3.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[3],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[3,10,12,13,1...|\n",
            "| 40|   technician|  single| tertiary|     no|   1646|    yes|  no|       1|  242|       2| failure|    yes|        1.0|            1.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,11,12,13,1...|\n",
            "| 44|   unemployed| married|secondary|     no|      0|    yes|  no|       1|  272|       1| failure|    yes|        7.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[7],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[7,10,12,13,1...|\n",
            "| 43|       admin.|  single|secondary|     no|   -247|    yes| yes|       2|   76|       4| failure|    yes|        2.0|            1.0|         1.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[2],[1.0])|          (2,[1],[1.0])|           (1,[],[])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[2,11,13,14,1...|\n",
            "| 54|   technician|divorced|secondary|     no|   2156|    yes|  no|       1|  195|       4| failure|    yes|        1.0|            2.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|              (2,[],[])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,12,13,14,1...|\n",
            "| 31|   management| married| tertiary|     no|   1331|     no|  no|       2|   90|       1| success|    yes|        0.0|            0.0|         0.0|            0.0|              1.0|            0.0|             1.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|               (1,[],[])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 28|self-employed| married| tertiary|     no|      0|    yes|  no|       3|  196|       4| failure|    yes|        8.0|            0.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[8],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[8,10,12,13,1...|\n",
            "| 32|       admin.|  single|secondary|     no|    678|    yes|  no|       1|  246|       4| failure|    yes|        2.0|            1.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[2],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[2,11,12,13,1...|\n",
            "| 41|self-employed|  single| tertiary|     no|    -94|    yes|  no|       1|  203|       1| failure|    yes|        8.0|            1.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[8],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[8,11,12,13,1...|\n",
            "| 31|   technician|  single| tertiary|     no|   1109|     no|  no|       1|  167|       2| failure|    yes|        1.0|            1.0|         0.0|            0.0|              1.0|            0.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,11,12,13,1...|\n",
            "| 26| entrepreneur|  single|secondary|     no|    101|    yes|  no|       1|  268|       1| failure|    yes|        9.0|            1.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[9],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[9,11,12,13,1...|\n",
            "| 31|   management|  single| tertiary|     no|  12857|    yes|  no|       1|   92|       1| success|    yes|        0.0|            1.0|         0.0|            0.0|              1.0|            1.0|             1.0|            0.0|     (10,[0],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[0,11,12,13,1...|\n",
            "| 33|  blue-collar|  single| tertiary|     no|    700|     no|  no|       1|   88|       1| success|    yes|        3.0|            1.0|         0.0|            0.0|              1.0|            0.0|             1.0|            0.0|     (10,[3],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|               (1,[],[])|          (1,[0],[1.0])|(19,[3,11,12,13,1...|\n",
            "| 34|       admin.| married|secondary|     no|      0|    yes|  no|       1|   85|       1| failure|    yes|        2.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[2],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[2,10,12,13,1...|\n",
            "| 40|  blue-collar| married|secondary|     no|   5060|     no|  no|       2|   93|       1| success|    yes|        3.0|            0.0|         0.0|            0.0|              0.0|            0.0|             1.0|            0.0|     (10,[3],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|          (1,[0],[1.0])|               (1,[],[])|          (1,[0],[1.0])|(19,[3,10,12,13,1...|\n",
            "| 37|       admin.| married|secondary|     no|   1314|    yes|  no|       4|  210|       3| failure|    yes|        2.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[2],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[2,10,12,13,1...|\n",
            "| 33|   management|  single| tertiary|     no|   3552|     no|  no|       2|   85|       3| failure|    yes|        0.0|            1.0|         0.0|            0.0|              1.0|            0.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,11,12,13,1...|\n",
            "| 30|   management|  single| tertiary|     no|   5561|    yes|  no|       1|  100|       1| success|    yes|        0.0|            1.0|         0.0|            0.0|              1.0|            1.0|             1.0|            0.0|     (10,[0],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[0,11,12,13,1...|\n",
            "| 60|      retired| married|secondary|     no|    404|     no|  no|       2|  196|       2| failure|    yes|        4.0|            0.0|         0.0|            0.0|              0.0|            0.0|             0.0|            0.0|     (10,[4],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[4,10,12,13,1...|\n",
            "| 31|   management|divorced| tertiary|     no|    883|    yes|  no|       2|  106|       1| failure|    yes|        0.0|            2.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[0],[1.0])|              (2,[],[])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,12,13,15,1...|\n",
            "| 44|   management|  single| tertiary|     no|    483|     no|  no|       2|  199|       6| success|    yes|        0.0|            1.0|         0.0|            0.0|              1.0|            0.0|             1.0|            0.0|     (10,[0],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|          (1,[0],[1.0])|               (1,[],[])|          (1,[0],[1.0])|(19,[0,11,12,13,1...|\n",
            "| 37|   unemployed|  single| tertiary|     no|   1316|    yes|  no|       2|  112|       3| failure|    yes|        7.0|            1.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[7],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[7,11,12,13,1...|\n",
            "| 37|     services|  single|secondary|     no|   1045|     no|  no|       2|   41|       1| failure|    yes|        5.0|            1.0|         0.0|            0.0|              0.0|            0.0|             0.0|            0.0|     (10,[5],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|          (1,[0],[1.0])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[5,11,12,13,1...|\n",
            "| 30|   management| married| tertiary|     no|   8089|    yes|  no|       2|  294|       2| failure|    yes|        0.0|            0.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 34|   technician| married|secondary|     no|    390|    yes|  no|       2|  231|       2| failure|    yes|        1.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "| 50|   management| married| tertiary|     no|    414|    yes|  no|       1|  302|       3| failure|    yes|        0.0|            0.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 55|  blue-collar|divorced|secondary|     no|   1613|    yes|  no|       1|  270|       3| failure|    yes|        3.0|            2.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[3],[1.0])|              (2,[],[])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[3,12,13,14,1...|\n",
            "| 41|self-employed| married|secondary|     no|   3123|    yes|  no|       2|  135|       2| failure|    yes|        8.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[8],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[8,10,12,13,1...|\n",
            "| 33|   technician|  single| tertiary|     no|    127|    yes|  no|       2|  222|      17| failure|    yes|        1.0|            1.0|         0.0|            0.0|              1.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[1],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[1],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,11,12,13,1...|\n",
            "| 37|       admin.| married|secondary|     no|   1207|    yes|  no|       2|  138|       2| success|    yes|        2.0|            0.0|         0.0|            0.0|              0.0|            1.0|             1.0|            0.0|     (10,[2],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|               (1,[],[])|          (1,[0],[1.0])|(19,[2,10,12,13,1...|\n",
            "| 48|   management| married|secondary|     no|   4415|    yes|  no|       3|  141|       7| failure|    yes|        0.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[0],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[0,10,12,13,1...|\n",
            "| 32|   technician| married|secondary|     no|    303|    yes|  no|       1|  267|       4| failure|    yes|        1.0|            0.0|         0.0|            0.0|              0.0|            1.0|             0.0|            0.0|     (10,[1],[1.0])|          (2,[0],[1.0])|       (1,[0],[1.0])|          (1,[0],[1.0])|            (2,[0],[1.0])|              (1,[],[])|           (1,[0],[1.0])|          (1,[0],[1.0])|(19,[1,10,12,13,1...|\n",
            "+---+-------------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+-----------+---------------+------------+---------------+-----------------+---------------+----------------+---------------+-------------------+-----------------------+--------------------+-----------------------+-------------------------+-----------------------+------------------------+-----------------------+--------------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETRsX1ujAyfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "job_indexer = StringIndexer(inputCol='job',outputCol='jobIndex')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnOj1CPlHd7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e117f860-4f38-46f8-b344-1d72619f9f23"
      },
      "source": [
        "job_encoder = OneHotEncoder(inputCol='jobIndex',outputCol='jobVec')\n",
        "job_encoder"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder_b50befd53d36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HilKVWIpCdAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "martial_indexer = StringIndexer(inputCol='marital',outputCol='maritalIndex')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fB_m9pHwSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "martial_encoder = OneHotEncoder(inputCol='maritalIndex',outputCol='maritalVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdPWCWG9Cxta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education_indexer = StringIndexer(inputCol='education',outputCol='educationIndex')\n",
        "education_encoder = OneHotEncoder(inputCol='educationIndex',outputCol='educationVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoeRpComD6L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_indexer = StringIndexer(inputCol='default',outputCol='defaultIndex')\n",
        "default_encoder = OneHotEncoder(inputCol='defaultIndex',outputCol='defaultVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlBVkiANDOAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_indexer = StringIndexer(inputCol='housing',outputCol='housingIndex')\n",
        "housing_encoder = OneHotEncoder(inputCol='housingIndex',outputCol='housingVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK3CR12fDet9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loan_indexer = StringIndexer(inputCol='loan',outputCol='loanIndex')\n",
        "loan_encoder = OneHotEncoder(inputCol='loanIndex',outputCol='loanVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl_Cem8NHQD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poutcome_indexer = StringIndexer(inputCol='poutcome',outputCol='poutcomeIndex')\n",
        "poutcome_encoder = OneHotEncoder(inputCol='poutcomeIndex',outputCol='poutcomeVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vleuOrA_IMJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deposit_indexer = StringIndexer(inputCol='deposit',outputCol='depositIndex')\n",
        "deposit_encoder = OneHotEncoder(inputCol='depositIndex',outputCol='depositVec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIGdsoDpAJ02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvbPJqCFE-S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assembler = VectorAssembler(inputCols=['age',\n",
        " 'jobVec',\n",
        " 'maritalVec',\n",
        " 'educationVec',\n",
        " 'defaultVec',\n",
        " 'balance',\n",
        " 'housingVec',\n",
        " 'loanVec',\n",
        " 'campaign',\n",
        " 'pdays',\n",
        " 'previous',\n",
        " 'poutcomeVec',\n",
        " 'depositVec'],outputCol='features')                         \n",
        "                                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMB6VMc1A3Id",
        "colab_type": "code",
        "outputId": "ab24eea9-18b1-432b-dee5-ea163d197c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "output=assembler.transform(sql_results)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1356.transform.\n: java.lang.IllegalArgumentException: Field \"jobVec\" does not exist.\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-bee22f634e47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"jobVec\" does not exist.\\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOqHERYJCQuC"
      },
      "source": [
        "## 3.1 normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FpA2EcYL0-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3=df2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cfL6_sca5VwI",
        "colab": {}
      },
      "source": [
        "#Apply Min-Max normalisation on each attribute using MinMaxScaler  \n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "#scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Gaz2i8I2yU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "1bc8eb47-9a0c-444b-e486-cd9a0b9a213c"
      },
      "source": [
        "norm_vars=['job_indexed_encoded','marital_indexed_encoded','loan_indexed_encoded','default_indexed_encoded','education_indexed_encoded','housing_indexed_encoded','poutcome_indexed_encoded','deposit_indexed_encoded']\n",
        "scaler=[MinMaxScaler(inputCol=scale_features,outputCol=scale_features+\"SCALED\")for scale_features in norm_vars]\n",
        "pipeline=Pipeline(stages=scaler)\n",
        "scalerModel=pipeline.fit(df3)\n",
        "scaledData=scalerModel.transform(df3)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2148.fit.\n: java.lang.IllegalArgumentException: Field \"job_indexed_encoded\" does not exist.\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)\n\tat org.apache.spark.ml.feature.MinMaxScalerParams$class.validateAndTransformSchema(MinMaxScaler.scala:67)\n\tat org.apache.spark.ml.feature.MinMaxScaler.validateAndTransformSchema(MinMaxScaler.scala:93)\n\tat org.apache.spark.ml.feature.MinMaxScaler.transformSchema(MinMaxScaler.scala:129)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:119)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-28fe3aacf0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_features\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"SCALED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mscale_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscalerModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscaledData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"job_indexed_encoded\" does not exist.\\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9trUY7ZgCzhW"
      },
      "source": [
        "# 4.Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KTQfUch2Cmmi"
      },
      "source": [
        "## 4.1 K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dGGZI70Ohqgg",
        "colab": {}
      },
      "source": [
        "# Perform unsupervised learning on df2 with k-means \n",
        "# You can use whole df2 as both training and testing data, \n",
        "# Evaluate the clustering result using Accuracy.  \n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nqaQEqrWjsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "kmeans_bank=KMeans(featuresCol='features').setK(2).setSeed(742)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUT5bLR0X5TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(stages=[job_indexer,martial_indexer,education_indexer,default_indexer,housing_indexer,loan_indexer,poutcome_indexer,\n",
        "                            job_encoder,martial_encoder,education_encoder,default_encoder,housing_encoder,loan_encoder,poutcome_encoder,\n",
        "                           assembler,kmeans_bank])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NI1KIKPYQGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_bank_data, test_bank_data = df2.randomSplit([0.7,.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFe88KSRYSrx",
        "colab_type": "code",
        "outputId": "302576da-1eab-4f22-f812-4795e439ce5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "fit_model = pipeline.fit(train_bank_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1500.fit.\n: java.lang.IllegalArgumentException: Field \"martial\" does not exist.\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit, jobIndex\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:85)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-153ebca5d7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bank_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"martial\" does not exist.\\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit, jobIndex'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoP_IaWuYXNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = fit_model.transform(test_bank_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLSxXrDYicZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
        "                                       labelCol='deposit')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts6Fi5shYlgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.select('deposit','prediction').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxs8J5XuYnzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUC = my_eval.evaluate(results)\n",
        "AUC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHom8o2KCt36"
      },
      "source": [
        "## 4.2 PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wT4cx5uGTjmj",
        "colab": {}
      },
      "source": [
        "#Generate a scatter plot using the first two PCA components to investigate the data distribution.\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKe5M54EDy-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(k=2, inputCol='features', outputCol='pcaFeature')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePQK-FLHEbo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.mllib.linalg.distributed import RowMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd3467odEfW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mat = RowMatrix(rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPzJv53FEf2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the top 4 principal components.\n",
        "# Principal components are stored in a local dense matrix.\n",
        "pc = mat.computePrincipalComponents(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLCkJwPAEjtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Project the rows to the linear space spanned by the top 4 principal components.\n",
        "projected = mat.multiply(pc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ulp_uILXCv4Z"
      },
      "source": [
        "# 5.Supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0O7tszcPfnHN",
        "colab": {}
      },
      "source": [
        "train, test = df2.randomSplit([0.7, 0.3], seed = 742)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2SsHdh7YC-eN"
      },
      "source": [
        "## 5.1 LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vqo_ywFQYxSj",
        "colab": {}
      },
      "source": [
        "# Logistic Regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW763bLDffQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_reg_bank = LogisticRegression(featuresCol='features',labelCol='deposit')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaFK4CFnfmKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(stages=[job_indexer,martial_indexer,education_indexer,default_indexer,housing_indexer,loan_indexer,poutcome_indexer,deposit_indexer,\n",
        "                            job_encoder,martial_encoder,education_encoder,default_encoder,housing_encoder,loan_encoder,poutcome_encoder,deposit_encoder,\n",
        "                           assembler,kmeans_bank])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSNP-_nFfsDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bank_data, test_bank_data = df2.randomSplit([0.7,.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfhpqm0ufvAE",
        "colab_type": "code",
        "outputId": "bdf39dc6-7c9f-4705-d3a8-085a627af1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "fit_model = pipeline.fit(train_bank_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o548.fit.\n: java.lang.IllegalArgumentException: Field \"martial\" does not exist.\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit, jobIndex\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:85)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-153ebca5d7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bank_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"martial\" does not exist.\\nAvailable fields: age, job, marital, education, default, balance, housing, loan, campaign, pdays, previous, poutcome, deposit, jobIndex'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPbx-HhOf3u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = fit_model.transform(test_bank_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRIL9cJYf8OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
        "                                       labelCol='deposit')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8TCM7hjf-pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.select('deposit','prediction').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgITBeB4gAvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUC = my_eval.evaluate(results)\n",
        "AUC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hANwFUzhgG83",
        "colab": {}
      },
      "source": [
        "#Exam the coefficients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evM5eiJoDHw2"
      },
      "source": [
        "## 5.2 Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "He4mlHb7hBoY",
        "colab": {}
      },
      "source": [
        "#Decision tree\n",
        "from pyspark.ml.classification import DecisionTreeClassifier "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cmp4-09zmEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtc=DecisionTreeClassifier(labelCol='deposit',featuresCol='features')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc-MELG-5PD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtcmodel=dtc.fit(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cejiC8V5VZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtc_predictions=dtcmodel.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VYpEIUY6EvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtc_evaluator=MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7JkenX6h0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtc_accuracy=dtc_evaluator.evaluate(dtc_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CaE-Z_IlDKXF"
      },
      "source": [
        "## 5.3 NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v2XL6I0t7irt",
        "colab": {}
      },
      "source": [
        "#NaiveBayes\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLS3H7er3jV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb=NaiveBayes(modelType=\"multinomial\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIueKsk4SEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbmodel=nb.fit(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp2svWhq4WYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_predictions=nbmodel.transform(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysi-FICH60qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_evaluator=MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUTXtDgl62kI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_accuracy=nb_evaluator.evaluate(nb_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}